{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c8dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importar funciones necesarias desde tu módulo de funciones\n",
    "from NB_funciones import preprocess_data, log_info, log_error, CargarDatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b885a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"dataset_name\": \"customer_support_twitter_twcs\",\n",
    "        \"text_column\": \"text\",\n",
    "        \"batch_size\": 100  # Tamaño de lote para predicciones\n",
    "    },\n",
    "    \"mlflow\": {\n",
    "        \"experiment_name\": \"experimento_nuevo_final\",\n",
    "        \"model_name\": \"modelo_nuevo\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_mlflow():\n",
    "    \"\"\"\n",
    "    Carga el modelo registrado en MLflow.\n",
    "\n",
    "    Esta función recupera la última versión del modelo especificado en MLflow y lo carga\n",
    "    utilizando `mlflow.sklearn.load_model`. Si ocurre un error, lo registra en los logs.\n",
    "\n",
    "    Args:\n",
    "        None (usa la configuración global `CONFIG` para obtener el nombre del modelo).\n",
    "\n",
    "    Returns:\n",
    "        sklearn model | None: Modelo cargado desde MLflow o `None` en caso de error.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Captura errores durante la carga y los registra en los logs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_uri = f\"models:/{CONFIG['mlflow']['model_name']}/latest\"\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "        log_info(f\"Modelo cargado desde {model_uri}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error al cargar el modelo desde MLflow: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdde51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_predicciones(model, datos_nuevos):\n",
    "    \"\"\"\n",
    "    Realiza predicciones con el modelo cargado.\n",
    "\n",
    "    Esta función toma un modelo previamente entrenado y realiza predicciones sobre un conjunto \n",
    "    de datos nuevos. Si ocurre un error durante la ejecución, se captura y registra en los logs.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado que se utilizará para generar predicciones.\n",
    "        datos_nuevos: Datos de entrada sobre los cuales se desea obtener predicciones.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray | list | None: Array o lista con las predicciones generadas, \n",
    "                                  `None` si ocurre un error.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Captura errores en la inferencia y los registra en los logs.\n",
    " \n",
    "    \"\"\"\n",
    "    try:\n",
    "        predicciones = model.predict(datos_nuevos)\n",
    "        return predicciones\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error al hacer predicciones: {e}\")\n",
    "        return None\n",
    "    \n",
    "def hacer_predicciones_por_lotes(model, datos_nuevos, batch_size=100):\n",
    "    \"\"\"\n",
    "     Realiza predicciones en lotes con el modelo cargado.\n",
    "\n",
    "    Esta función divide los datos de entrada en bloques (`batch_size`) y genera predicciones \n",
    "    en cada iteración para optimizar el procesamiento en modelos grandes. Es útil cuando \n",
    "    el conjunto de datos es extenso y no puede procesarse de una sola vez.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado que se utilizará para generar predicciones.\n",
    "        datos_nuevos (pd.DataFrame): Conjunto de datos sobre el cual se desean obtener predicciones.\n",
    "        batch_size (int, opcional): Tamaño del lote de datos procesados en cada iteración. Por defecto `100`.\n",
    "\n",
    "    Returns:\n",
    "        list | None: Lista con todas las predicciones generadas, `None` en caso de error.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Captura errores en la inferencia y los registra en los logs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predicciones_totales = []\n",
    "        \n",
    "        for i in range(0, len(datos_nuevos), batch_size):\n",
    "            batch = datos_nuevos.iloc[i:i + batch_size]\n",
    "            predicciones = model.predict(batch)\n",
    "            predicciones_totales.extend(predicciones)\n",
    "        \n",
    "        return predicciones_totales\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error al hacer predicciones por lotes: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f03d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Ejecuta el flujo de predicción en lotes.\n",
    "\n",
    "    Este pipeline carga el modelo desde MLflow, obtiene nuevos datos desde un archivo CSV \n",
    "    y genera predicciones por lotes para optimizar la inferencia en conjuntos de datos grandes.\n",
    "\n",
    "    Returns:\n",
    "        None: No devuelve valores explícitos, pero registra información relevante en los logs.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Captura errores en cualquier etapa y los registra en los logs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_info(\"Iniciando flujo de predicción en lotes...\")\n",
    "\n",
    "        # Cargar modelo\n",
    "        model = cargar_modelo_mlflow()\n",
    "        if model is None:\n",
    "            log_error(\"No se pudo cargar el modelo. Deteniendo ejecución.\")\n",
    "            return\n",
    "        \n",
    "        # Cargar datos nuevos\n",
    "        ruta = CargarDatos(CONFIG[\"data\"][\"dataset_name\"])\n",
    "        datos_nuevos = ruta.cargar_csv()\n",
    "        log_info(f\"Datos nuevos cargados con {datos_nuevos.shape[0]} registros.\")\n",
    "\n",
    "        # Generar predicciones por lotes\n",
    "        predicciones = hacer_predicciones_por_lotes(model, datos_nuevos['text'], CONFIG[\"data\"][\"batch_size\"])\n",
    "        if predicciones is not None:\n",
    "            log_info(f\"Predicciones generadas exitosamente: {predicciones[:10]}\")  # Mostramos solo 10 ejemplos\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Error crítico en el flujo de predicción por lotes: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b707b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Iniciando flujo de predicción en lotes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Modelo cargado desde models:/modelo_nuevo/latest\n",
      "INFO:root:Archivo /home/alejo/proyectos/MLE_Nequi/datasets/Customer_Support_Twitter/twcs/twcs.csv cargado correctamente.\n",
      "INFO:root:Datos nuevos cargados con 2811774 registros.\n",
      "INFO:root:Predicciones generadas exitosamente: [np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8378627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook NB_prediccion_final.ipynb to script\n",
      "[NbConvertApp] Writing 5237 bytes to NB_prediccion_final.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script NB_prediccion_final.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
