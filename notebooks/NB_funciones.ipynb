{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02886b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "    \n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, fbeta_score, roc_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import string \n",
    "import joblib\n",
    "import warnings\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlflow.models.signature import infer_signature\n",
    "from datetime import datetime\n",
    "    \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f2f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para registrar mensajes tanto en log como en consola\n",
    "def log_info(message):\n",
    "    print(message)\n",
    "    logging.info(message)\n",
    "        \n",
    "def log_error(message):\n",
    "    print(f'ERROR: {message}')\n",
    "    logging.error(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log_info = logging.info\n",
    "log_error = logging.error\n",
    "\n",
    "class CargarDatos:\n",
    "    \"\"\"\n",
    "    Clase para cargar datos desde un archivo CSV protegido con .env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clave_env):\n",
    "        # Cargar las variables de entorno\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Obtener la ruta desde el archivo .env\n",
    "        self.ruta_archivo = os.getenv(clave_env)\n",
    "        if not self.ruta_archivo:\n",
    "            raise ValueError(f\"La clave '{clave_env}' no está definida en .env\")\n",
    "        \n",
    "        self.df = None\n",
    "\n",
    "    def cargar_csv(self):\n",
    "        \"\"\"Carga el archivo CSV y lo almacena en un DataFrame\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.ruta_archivo)\n",
    "            log_info(f\"Archivo {self.ruta_archivo} cargado correctamente.\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error al cargar el archivo {self.ruta_archivo}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14aaf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    '''\n",
    "    Clase para preprocesar texto.\n",
    "    Atributos:\n",
    "        stop_words (set): Conjunto de palabras vacías en ingles.\n",
    "        nlp (spacy.lang.en.EN): Modelo de lenguaje en ingles de spaCy.\n",
    "    Métodos:\n",
    "        __init__(): Inicializa el preprocesador de texto.\n",
    "        limpiar_texto(): Limpia el texto eliminando caracteres no deseados.\n",
    "        tokenizar_texto(): Tokeniza el texto y elimina palabras vacías.\n",
    "        lematizar_texto(): Lematiza el texto utilizando spaCy.\n",
    "        preprocesar_texto(): Preprocesa el texto completo.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def limpiar_texto(self, texto):\n",
    "        # Eliminar caracteres no deseados\n",
    "        try:\n",
    "            texto = texto.lower()\n",
    "            texto = re.sub(r'\\d+', '', texto)  # Eliminar números\n",
    "            texto = re.sub(r'\\s+', ' ', texto)  # Eliminar espacios extra\n",
    "            texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)  # Eliminar URLs\n",
    "            texto = re.sub(r'@\\w+', '', texto)  # Eliminar menciones\n",
    "            texto = re.sub(r'#', '', texto)  # Eliminar hashtags\n",
    "            texto = re.sub(r'[^a-zA-Z0-9áéíóúüñÑÁÉÍÓÚÜ\\s]', '', texto)\n",
    "            log_info('Texto limpiado correctamente.')\n",
    "        except Exception as e:\n",
    "            log_error(f'Error al limpiar el texto: {e}')\n",
    "            raise e\n",
    "        return texto\n",
    "\n",
    "    def tokenizar_texto(self, texto):\n",
    "        # Tokenizar y eliminar palabras vacías\n",
    "        try:\n",
    "            tokens = word_tokenize(texto.lower())\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "            log_info('Tokens generados correctamente.')\n",
    "        except Exception as e:\n",
    "            log_error(f'Error al tokenizar el texto: {e}')\n",
    "            raise e\n",
    "        return tokens\n",
    "\n",
    "    def lematizar_texto(self, tokens):\n",
    "        # Lematizar los tokens\n",
    "        try:\n",
    "            doc = self.nlp(' '.join(tokens))\n",
    "            lemas = [token.lemma_ for token in doc]\n",
    "            lemas = [lema for lema in lemas if lema not in self.stop_words]\n",
    "            lemas = [lema for lema in lemas if len(lema) > 1]  # Eliminar lemas de longitud 1\n",
    "            lemas = [lema for lema in lemas if not re.match(r'^[a-zA-Z0-9]+$', lema)]  # Eliminar lemas que son solo números\n",
    "            log_info('Lematización completada correctamente.')\n",
    "        except Exception as e:\n",
    "            log_error(f'Error al lematizar el texto: {e}')\n",
    "            raise e\n",
    "        return lemas\n",
    "\n",
    "    def preprocesar_texto(self, texto):\n",
    "        # Preprocesar el texto completo\n",
    "        try:\n",
    "            texto_limpio = self.limpiar_texto(texto)\n",
    "            tokens = self.tokenizar_texto(texto_limpio)\n",
    "            lemas = self.lematizar_texto(tokens)\n",
    "            log_info('Texto preprocesado correctamente.')\n",
    "        except Exception as e:\n",
    "            log_error(f'Error al preprocesar el texto: {e}')\n",
    "            raise e\n",
    "        return ' '.join(lemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ae9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_experiment(X_train, y_train, X_test, y_test, english_stopwords, model_name=\"modelo_catboost_prueba\"):\n",
    "    \"\"\"\n",
    "    Ejecuta un experimento de machine learning con CatBoost y MLflow.\n",
    "\n",
    "    Parámetros:\n",
    "    - X_train, y_train: Datos de entrenamiento.\n",
    "    - X_test, y_test: Datos de prueba.\n",
    "    - english_stopwords: Lista de palabras irrelevantes para la vectorización.\n",
    "    - model_name: Nombre para registrar el modelo en MLflow.\n",
    "\n",
    "    Registra el modelo, métricas y parámetros en MLflow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Iniciando el experimento...\")\n",
    "        \n",
    "        # Definir los hiperparámetros del modelo\n",
    "        catboost_params = {\n",
    "            \"iterations\": 500,\n",
    "            \"depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"verbose\": 100\n",
    "        }\n",
    "\n",
    "        # Construir el pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=100, lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "            ('catboost', CatBoostClassifier(**catboost_params))\n",
    "        ])\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Entrenar el modelo\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Inferir la firma del modelo\n",
    "            signature = infer_signature(X_train, y_train)\n",
    "\n",
    "            # Registrar el modelo en MLflow\n",
    "            input_example = np.array(X_train[:1])  \n",
    "            mlflow.sklearn.log_model(pipeline, \"modelo_catboost\", \n",
    "                                     input_example=input_example, \n",
    "                                     signature=signature, \n",
    "                                     registered_model_name=model_name)\n",
    "\n",
    "            # Registrar métricas y parámetros\n",
    "            accuracy = pipeline.score(X_test, y_test)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_params(catboost_params)\n",
    "\n",
    "            print(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "            logging.info(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error durante la ejecución de MLflow: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d99a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X, y, title='Matriz de Confusión'):\n",
    "    '''Grafica la matriz de confusión para un modelo.'''\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ConfusionMatrixDisplay.from_estimator(model, X, y, ax=ax)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al graficar matriz de confusión: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92efb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, X, y, model_name='Modelo'):\n",
    "    '''Grafica la curva ROC para un modelo.'''\n",
    "    try:\n",
    "        if not hasattr(model, 'predict_proba'):\n",
    "            log_info(f'El modelo {model_name} no soporta predict_proba, no se puede graficar ROC.')\n",
    "            return None\n",
    "                \n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Curva ROC')\n",
    "        ax.legend(loc='lower right')\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al graficar curva ROC: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c009813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_model(model_name='Modelo'):\n",
    "    '''Recupera el modelo campeón actual desde MLflow.'''\n",
    "    try:\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "            \n",
    "        # Buscar la última versión del modelo campeón\n",
    "        try:\n",
    "            latest_version = client.get_latest_versions(model_name, stages=['Production'])\n",
    "            if not latest_version:\n",
    "                log_info(f'No se encontró un modelo {model_name} en producción. Buscando en staging...')\n",
    "                latest_version = client.get_latest_versions(model_name, stages=['Staging'])\n",
    "                    \n",
    "            if not latest_version:\n",
    "                log_info(f'No se encontró un modelo {model_name} en staging. Buscando la versión más reciente...')\n",
    "                latest_version = client.get_latest_versions(model_name)\n",
    "                    \n",
    "            if latest_version:\n",
    "                model_uri = f'models:/{model_name}/{latest_version[0].version}'\n",
    "                champion_model = mlflow.sklearn.load_model(model_uri)\n",
    "                log_info(f'Modelo campeón cargado: {model_name} version {latest_version[0].version}')\n",
    "                return champion_model, latest_version[0].run_id\n",
    "            else:\n",
    "                log_info(f'No se encontró ningún modelo registrado con el nombre {model_name}')\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            log_error(f'No se pudo obtener la última versión del modelo: {e}')\n",
    "            return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f'Error al recuperar el modelo campeón: {e}')\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd76680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_challenger_model(model, metrics, X_train, y_train, is_champion=False, CHAMPION_MODEL_NAME='Modelo_Campeon'):\n",
    "    '''Registra un modelo desafiante en MLflow.'''\n",
    "    try:\n",
    "        # Inferir firma para input/output\n",
    "        signature = infer_signature(X_train, y_train)\n",
    "            \n",
    "        # Registrar el modelo con un ejemplo de entrada\n",
    "        input_example = np.array(X_train[:1])\n",
    "            \n",
    "        # Nombre del modelo y etapa\n",
    "        model_name = CHAMPION_MODEL_NAME if is_champion else f'{CHAMPION_MODEL_NAME}_challenger'\n",
    "        stage = 'Production' if is_champion else 'Staging'\n",
    "            \n",
    "        # Registrar modelo\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            'model', \n",
    "            input_example=input_example, \n",
    "            signature=signature, \n",
    "            registered_model_name=model_name\n",
    "        )\n",
    "            \n",
    "        # Registrar métricas\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "        # Si es el campeón, mover a producción\n",
    "        if is_champion:\n",
    "            client = mlflow.tracking.MlflowClient()\n",
    "            latest_version = client.get_latest_versions(model_name)[0].version\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=latest_version,\n",
    "                stage=stage\n",
    "            )\n",
    "            log_info(f'Modelo {model_name} v{latest_version} promocionado a {stage}')\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al registrar el modelo: {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e317ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_challenger_model(X_train, y_train, X_valid=None, y_valid=None, hyperparams=None):\n",
    "    '''Entrena un nuevo modelo desafiante.'''\n",
    "    try:\n",
    "        # Parámetros por defecto si no se especifican\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {\n",
    "                'iterations': 500,\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'verbose': 100,\n",
    "                'max_features': 100\n",
    "            }\n",
    "            \n",
    "        # Extraer parámetros específicos de vectorizador y modelo\n",
    "        max_features = hyperparams.pop('max_features', 100)\n",
    "            \n",
    "        log_info(f'Entrenando modelo desafiante con parámetros: {hyperparams}')\n",
    "        \n",
    "        english_stopwords = stopwords.words('english')\n",
    "\n",
    "        # Crear pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=max_features, lowercase=True, token_pattern=r'\\b\\w+\\b')),              \n",
    "            ('catboost', CatBoostClassifier(**hyperparams))\n",
    "        ])\n",
    "            \n",
    "        # Entrenar modelo\n",
    "        if X_valid is not None and y_valid is not None:\n",
    "            # Usar conjunto de validación para early stopping\n",
    "            pipeline.fit(X_train, y_train, catboost__eval_set=[(X_valid, y_valid)])\n",
    "        else:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "        log_info('Modelo desafiante entrenado correctamente')\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al entrenar modelo desafiante: {e}')\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2627c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(champion_metrics, challenger_metrics, primary_metric=\"accuracy\", threshold=0.5):\n",
    "    '''Compara los modelos y determina si el desafiante debe convertirse en el nuevo campeón.'''\n",
    "    try:\n",
    "        if champion_metrics is None:\n",
    "            log_info('No hay modelo campeón para comparar. El desafiante se convierte en campeón por defecto.')\n",
    "            return True, {}, {'champion': None, 'challenger': challenger_metrics[primary_metric]}\n",
    "            \n",
    "        # Comparar métricas primarias\n",
    "        champion_score = champion_metrics[primary_metric]\n",
    "        challenger_score = challenger_metrics[primary_metric]\n",
    "            \n",
    "        improvement = challenger_score - champion_score\n",
    "        percent_improvement = (improvement / champion_score) * 100 if champion_score > 0 else float('inf')\n",
    "            \n",
    "        comparison = {\n",
    "            'champion': champion_score,\n",
    "            'challenger': challenger_score,\n",
    "            'absolute_diff': improvement,\n",
    "            'percent_diff': percent_improvement\n",
    "        }\n",
    "            \n",
    "        # Comparar todas las métricas disponibles\n",
    "        all_metrics = {}\n",
    "        for metric in set(champion_metrics.keys()).union(challenger_metrics.keys()):\n",
    "            if metric in champion_metrics and metric in challenger_metrics:\n",
    "                champion_val = champion_metrics[metric]\n",
    "                challenger_val = challenger_metrics[metric]\n",
    "                diff = challenger_val - champion_val\n",
    "                perc_diff = (diff / champion_val) * 100 if champion_val > 0 else float('inf')\n",
    "                    \n",
    "                all_metrics[metric] = {\n",
    "                    'champion': champion_val,\n",
    "                    'challenger': challenger_val,\n",
    "                    'absolute_diff': diff,\n",
    "                    'percent_diff': perc_diff\n",
    "                }\n",
    "            \n",
    "        # Determinar si el desafiante es mejor\n",
    "        is_better = improvement > threshold\n",
    "            \n",
    "        if is_better:\n",
    "            log_info(f'El modelo desafiante es mejor en {primary_metric}: {challenger_score:.4f} vs {champion_score:.4f} ')\n",
    "            log_info(f'Mejora absoluta: {improvement:.4f}, Mejora porcentual: {percent_improvement:.2f}%')\n",
    "        else:\n",
    "            log_info(f'El modelo desafiante NO supera al campeón en {primary_metric}: {challenger_score:.4f} vs {champion_score:.4f}')\n",
    "            log_info(f'Diferencia absoluta: {improvement:.4f}, Diferencia porcentual: {percent_improvement:.2f}%')\n",
    "                \n",
    "        return is_better, all_metrics, comparison\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al comparar modelos: {e}')\n",
    "        return False, {}, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment(log_file=\"training.log\"):\n",
    "    \"\"\"Configura el entorno de ejecución: logging, warnings, descargas NLTK\"\"\"\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Configuración de logging\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    \n",
    "    # Descargar recursos NLTK necesarios\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        log_info(\"Recursos NLTK descargados correctamente\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error descargando recursos NLTK: {e}\")\n",
    "        \n",
    "    return stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64188a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(data, text_column, target_column, test_size=0.3, valid_size=0.3, random_state=42):\n",
    "    \"\"\"Preprocesa los datos y crea conjuntos de entrenamiento, validación y prueba\"\"\"\n",
    "    try:\n",
    "        # Asegurar que el target sea int\n",
    "        data[target_column] = data[target_column].astype('int')\n",
    "        \n",
    "        # División inicial train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data[text_column], \n",
    "            data[target_column], \n",
    "            test_size=test_size, \n",
    "            stratify=data[target_column], \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # División adicional para conjunto de validación\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            test_size=valid_size, \n",
    "            stratify=y_train, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        log_info(f'Datos divididos: Train {X_train.shape[0]}, Valid {X_valid.shape[0]}, Test {X_test.shape[0]} registros')\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train, 'y_train': y_train,\n",
    "            'X_valid': X_valid, 'y_valid': y_valid,\n",
    "            'X_test': X_test, 'y_test': y_test\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_error(f'Error en el preprocesamiento de datos: {e}')\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf416d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(experiment_name):\n",
    "    \"\"\"Crea o configura un experimento MLflow\"\"\"\n",
    "    try:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        log_info(f\"Experimento '{experiment_name}' creado exitosamente\")\n",
    "    except:\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        log_info(f\"Usando experimento existente '{experiment_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"Evalúa un modelo y devuelve métricas detalladas\"\"\"\n",
    "    try:\n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Métricas básicas\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        }\n",
    "        \n",
    "        # Métricas avanzadas si disponemos de probabilidades\n",
    "        if y_proba is not None:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "            metrics[\"average_precision\"] = average_precision_score(y_test, y_proba)\n",
    "        \n",
    "        log_info(f\"Evaluación completa del modelo - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}\")\n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_error(f\"Error evaluando el modelo: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98946c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts(model, model_name, version, metrics, data_signature=None):\n",
    "    \"\"\"Guarda artefactos del modelo (gráficos, métricas, etc)\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"model_artifacts/{model_name}/{version}_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Guardar métricas en JSON\n",
    "        with open(f\"{output_dir}/metrics.json\", 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "            \n",
    "        # Guardar modelo usando joblib (como backup adicional a MLflow)\n",
    "        joblib.dump(model, f\"{output_dir}/{model_name}_{version}.joblib\")\n",
    "        \n",
    "        # Guardar firma del modelo si está disponible\n",
    "        if data_signature:\n",
    "            with open(f\"{output_dir}/data_signature.json\", 'w') as f:\n",
    "                json.dump(data_signature, f, indent=2)\n",
    "        \n",
    "        log_info(f\"Artefactos del modelo guardados en {output_dir}\")\n",
    "        return output_dir\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error guardando artefactos del modelo: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c65044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(model_info, metrics, output_path):\n",
    "    \"\"\"Genera una tarjeta de modelo (model card) con información clave\"\"\"\n",
    "    try:\n",
    "        model_card = {\n",
    "            \"model_name\": model_info[\"name\"],\n",
    "            \"version\": model_info[\"version\"],\n",
    "            \"description\": model_info.get(\"description\", \"\"),\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"performance_metrics\": metrics,\n",
    "            \"training_data\": model_info.get(\"training_data\", {}),\n",
    "            \"parameters\": model_info.get(\"parameters\", {}),\n",
    "            \"limitations\": model_info.get(\"limitations\", \"\"),\n",
    "            \"intended_use\": model_info.get(\"intended_use\", \"\"),\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(model_card, f, indent=2)\n",
    "            \n",
    "        log_info(f\"Model card generada en {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error generando model card: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_drift_detection(reference_data, current_data, columns=None, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detecta data drift entre dos conjuntos de datos\n",
    "    \n",
    "    Args:\n",
    "        reference_data: DataFrame con datos de referencia (entrenamiento original)\n",
    "        current_data: DataFrame con datos actuales\n",
    "        columns: Lista de columnas a analizar (None = todas)\n",
    "        threshold: Umbral para considerar drift significativo\n",
    "        \n",
    "    Returns:\n",
    "        Dict con resultados de análisis de drift\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if columns is None:\n",
    "            # Si no se especifican columnas, usar todas las comunes entre ambos dataframes\n",
    "            columns = [col for col in reference_data.columns if col in current_data.columns]\n",
    "        \n",
    "        results = {\n",
    "            \"drift_detected\": False,\n",
    "            \"details\": {}\n",
    "        }\n",
    "        \n",
    "        for col in columns:\n",
    "            # Simplificación: usando estadísticas básicas para detectar drift\n",
    "            # En una implementación real, usar métodos como KS test o PSI (Population Stability Index)\n",
    "            if pd.api.types.is_numeric_dtype(reference_data[col]):\n",
    "                ref_mean = reference_data[col].mean()\n",
    "                cur_mean = current_data[col].mean()\n",
    "                \n",
    "                ref_std = reference_data[col].std()\n",
    "                cur_std = current_data[col].std()\n",
    "                \n",
    "                # Calcular cambio relativo\n",
    "                mean_change = abs(ref_mean - cur_mean) / (abs(ref_mean) if abs(ref_mean) > 0 else 1)\n",
    "                std_change = abs(ref_std - cur_std) / (abs(ref_std) if abs(ref_std) > 0 else 1)\n",
    "                \n",
    "                col_drift = mean_change > threshold or std_change > threshold\n",
    "                \n",
    "                results[\"details\"][col] = {\n",
    "                    \"drift_detected\": col_drift,\n",
    "                    \"metrics\": {\n",
    "                        \"mean_change\": mean_change,\n",
    "                        \"std_change\": std_change\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                if col_drift:\n",
    "                    results[\"drift_detected\"] = True\n",
    "            \n",
    "            elif pd.api.types.is_categorical_dtype(reference_data[col]) or pd.api.types.is_object_dtype(reference_data[col]):\n",
    "                # Para variables categóricas, comparar distribución de valores\n",
    "                ref_dist = reference_data[col].value_counts(normalize=True).to_dict()\n",
    "                cur_dist = current_data[col].value_counts(normalize=True).to_dict()\n",
    "                \n",
    "                # Calcular diferencia en distribuciones\n",
    "                all_categories = set(ref_dist.keys()) | set(cur_dist.keys())\n",
    "                max_diff = 0\n",
    "                \n",
    "                for cat in all_categories:\n",
    "                    ref_val = ref_dist.get(cat, 0)\n",
    "                    cur_val = cur_dist.get(cat, 0)\n",
    "                    diff = abs(ref_val - cur_val)\n",
    "                    max_diff = max(max_diff, diff)\n",
    "                \n",
    "                col_drift = max_diff > threshold\n",
    "                \n",
    "                results[\"details\"][col] = {\n",
    "                    \"drift_detected\": col_drift,\n",
    "                    \"metrics\": {\n",
    "                        \"max_category_difference\": max_diff\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                if col_drift:\n",
    "                    results[\"drift_detected\"] = True\n",
    "        \n",
    "        log_info(f\"Análisis de data drift completado. Drift detectado: {results['drift_detected']}\")\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_error(f\"Error en detección de data drift: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95cb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_training_pipeline(\n",
    "    data, text_column, target_column, experiment_name, model_name, version=\"1.0.0\",\n",
    "    test_size=0.3, valid_size=0.3, random_state=42, optimization_metric=\"f1_score\",\n",
    "    champion_threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo de entrenamiento de un modelo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Configurar entorno\n",
    "        stopwords_list = setup_environment()\n",
    "        \n",
    "        # 2. Preparar datos\n",
    "        data_splits = preprocess_data(\n",
    "            data, text_column, target_column, \n",
    "            test_size=test_size, valid_size=valid_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # 3. Configurar experimento MLflow\n",
    "        create_mlflow_experiment(experiment_name)\n",
    "        \n",
    "        # 4. Entrenar modelo y registrar experimento\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_v{version}\") as run:\n",
    "            # Ejecutar experimento existente\n",
    "            model = run_ml_experiment(\n",
    "                data_splits['X_train'], data_splits['y_train'],\n",
    "                data_splits['X_test'], data_splits['y_test'],\n",
    "                stopwords_list, model_name=model_name\n",
    "            )\n",
    "            \n",
    "            # Evaluar modelo en conjunto de prueba\n",
    "            metrics = evaluate_model(model, data_splits['X_test'], data_splits['y_test'])\n",
    "            \n",
    "            # Registrar métricas en MLflow\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "            # Inferir firma del modelo\n",
    "            signature = infer_signature(\n",
    "                data_splits['X_train'].iloc[:5], \n",
    "                model.predict(data_splits['X_train'].iloc[:5])\n",
    "            )\n",
    "            \n",
    "            # Registrar modelo en MLflow\n",
    "            mlflow.sklearn.log_model(\n",
    "                model, \n",
    "                model_name, \n",
    "                signature=signature,\n",
    "                registered_model_name=model_name\n",
    "            )\n",
    "            \n",
    "            # Guardar artefactos adicionales\n",
    "            artifacts_dir = save_model_artifacts(\n",
    "                model, model_name, version, metrics, \n",
    "                data_signature={\"input_example\": data_splits['X_train'].iloc[0]}\n",
    "            )\n",
    "            \n",
    "            # Generar model card\n",
    "            model_info = {\n",
    "                \"name\": model_name,\n",
    "                \"version\": version,\n",
    "                \"description\": f\"Modelo de clasificación de texto entrenado con {data_splits['X_train'].shape[0]} ejemplos\",\n",
    "                \"training_data\": {\n",
    "                    \"rows\": data.shape[0],\n",
    "                    \"columns\": data.shape[1],\n",
    "                    \"date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                },\n",
    "                \"parameters\": model.get_params() if hasattr(model, 'get_params') else {}\n",
    "            }\n",
    "            \n",
    "            if artifacts_dir:\n",
    "                generate_model_card(model_info, metrics, f\"{artifacts_dir}/model_card.json\")\n",
    "        \n",
    "        # 5. Comparar con modelo campeón si existe\n",
    "        try:\n",
    "            champion_model = get_champion_model(model_name=model_name)\n",
    "            if champion_model:\n",
    "                champion_metrics = evaluate_model(champion_model, data_splits['X_test'], data_splits['y_test'])\n",
    "                comparison_result = compare_models(\n",
    "                    champion_metrics=champion_metrics,\n",
    "                    challenger_metrics=metrics,\n",
    "                    primary_metric=optimization_metric,\n",
    "                    threshold=champion_threshold\n",
    "                )\n",
    "                log_info(f\"Comparación de modelos: {comparison_result}\")\n",
    "        except Exception as e:\n",
    "            log_info(f\"No se encontró modelo campeón o error en comparación: {e}\")\n",
    "        \n",
    "        return model, metrics, run.info.run_id\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_error(f\"Error en pipeline de entrenamiento: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook NB_funciones.ipynb to script\n",
      "[NbConvertApp] Writing 15649 bytes to NB_funciones.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script NB_funciones.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
