{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, fbeta_score, roc_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import string \n",
    "import joblib\n",
    "import warnings\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d76677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    filename=\"errores_entrenamiento.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    load_dotenv()\n",
    "    ruta_cst_twcs = os.getenv(\"customer_support_twitter_twcs\")\n",
    "    logging.info(\"Environment variables loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading environment variables: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3be8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "try:\n",
    "    data_cst_twcs = pd.read_csv(ruta_cst_twcs)\n",
    "    print(data_cst_twcs.shape)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"File not found: {ruta_cst_twcs}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the 'inbound' column to int\n",
    "try:\n",
    "    data_cst_twcs['inbound'] = data_cst_twcs['inbound'].astype('int')\n",
    "    logging.info(\"Data transformed successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error transforming data: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3659f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    #nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    english_stopwords = stopwords.words('english')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading stopwords: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validation and test sets\n",
    "# stratified split to maintain the same proportion of classes in each set\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cst_twcs['text'], data_cst_twcs['inbound'], test_size=0.3, stratify=data_cst_twcs['inbound'], random_state=42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n",
    "    print('X_train: ', X_train.shape)\n",
    "    print('X_valid: ', X_valid.shape)\n",
    "    print('X_test: ', X_test.shape)\n",
    "    print('y_train: ', y_train.shape)\n",
    "    print('y_valid: ', y_valid.shape)\n",
    "    print('y_test: ', y_test.shape)\n",
    "    logging.info(\"Data split into train, validation and test sets successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error splitting data: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2efb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar el texto con TF-IDF\n",
    "#vectorizer = TfidfVectorizer(stop_words=english_stopwords, max_features=100, lowercase=True, token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the train, validation and test sets\n",
    "#X_train = vectorizer.fit_transform(X_train)\n",
    "#X_valid = vectorizer.transform(X_valid)\n",
    "#X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ea524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "try:\n",
    "    mlflow.create_experiment(\"experimento_catboost\")\n",
    "    print(\"Experimento creado\")\n",
    "    logging.info(\"Experiment created successfully.\")\n",
    "except:\n",
    "    mlflow.set_experiment(\"experimento_catboost\")\n",
    "    print(\"Experimento ya existe\")\n",
    "    logging.info(\"Experiment already exists, set to existing experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Iniciando el experimento...\")\n",
    "    with mlflow.start_run():\n",
    "        # Define y entrena el pipeline\n",
    "        catboost_params = {\n",
    "            \"iterations\": 500,\n",
    "            \"depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"verbose\": 100\n",
    "        }\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=100, lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "            ('catboost', CatBoostClassifier(**catboost_params))\n",
    "        ])\n",
    "        pipeline.fit(X_valid, y_valid)\n",
    "\n",
    "        # Inferir signature para input/output\n",
    "        signature = infer_signature(X_valid, y_valid)\n",
    "\n",
    "        # Registra el modelo con un ejemplo de entrada\n",
    "        input_example = np.array(X_train[:1])  # Toma una muestra como ejemplo de entrada\n",
    "        mlflow.sklearn.log_model(pipeline, \"modelo_catboost\", input_example=input_example, signature=signature, registered_model_name=\"modelo_catboost_prueba\")\n",
    "\n",
    "        # Registra las métricas\n",
    "        accuracy = pipeline.score(X_test, y_test)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Registra los hiperparámetros del modelo\n",
    "        mlflow.log_params(catboost_params)\n",
    "\n",
    "        print(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "        logging.info(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during MLflow run: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5f8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "488eff33",
   "metadata": {},
   "source": [
    "# MLOps - Proceso de Reentrenamiento y Comparación de Modelos\n",
    "\n",
    "Este notebook implementa un proceso MLOps para:\n",
    "1. Entrenar un nuevo modelo (challenger)\n",
    "2. Recuperar el modelo campeón actual de MLflow (champion)\n",
    "3. Comparar ambos modelos usando métricas definidas\n",
    "4. Registrar como nuevo campeón el que tenga mejor desempeño\n",
    "5. Documentar toda la experimentación en MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e445be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "    \n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, fbeta_score, roc_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import string \n",
    "import joblib\n",
    "import warnings\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlflow.models.signature import infer_signature\n",
    "from datetime import datetime\n",
    "    \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b96135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    filename=f'reentrenamiento_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee9f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para registrar mensajes tanto en log como en consola\n",
    "def log_info(message):\n",
    "    print(message)\n",
    "    logging.info(message)\n",
    "        \n",
    "def log_error(message):\n",
    "    print(f'ERROR: {message}')\n",
    "    logging.error(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c36b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entorno cargadas correctamente.\n",
      "Variables de entorno cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno\n",
    "try:\n",
    "    load_dotenv()\n",
    "    ruta_cst_twcs = os.getenv('customer_support_twitter_twcs')\n",
    "    # Configurar nombre del modelo campeón y experimento\n",
    "    CHAMPION_MODEL_NAME = os.getenv('CHAMPION_MODEL_NAME', 'modelo_catboost_champion')\n",
    "    EXPERIMENT_NAME = os.getenv('EXPERIMENT_NAME', 'experimento_catboost')\n",
    "    # Configurar umbral para considerar un modelo mejor\n",
    "    IMPROVEMENT_THRESHOLD = float(os.getenv('IMPROVEMENT_THRESHOLD', '0.01'))  # 1% de mejora por defecto\n",
    "    # Métrica principal para comparación\n",
    "    PRIMARY_METRIC = os.getenv('PRIMARY_METRIC', 'f1')\n",
    "    print('Variables de entorno cargadas correctamente.')\n",
    "    log_info('Variables de entorno cargadas correctamente.')\n",
    "except Exception as e:\n",
    "    print(f'Error al cargar variables de entorno: {e}')\n",
    "    log_error(f'Error al cargar variables de entorno: {e}')\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f49657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (2811774, 7)\n",
      "Datos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "try:\n",
    "    data_cst_twcs = pd.read_csv(ruta_cst_twcs)\n",
    "    print(f'Dimensiones del dataset: {data_cst_twcs.shape}')\n",
    "    log_info('Datos cargados correctamente.')\n",
    "except FileNotFoundError as e:\n",
    "    log_error(f'Archivo no encontrado: {ruta_cst_twcs}')\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71605b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos transformados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Transformar la columna 'inbound' a int\n",
    "try:\n",
    "    data_cst_twcs['inbound'] = data_cst_twcs['inbound'].astype('int')\n",
    "    log_info('Datos transformados correctamente.')\n",
    "except Exception as e:\n",
    "    log_error(f'Error al transformar datos: {e}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb3d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursos NLP cargados correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alejo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alejo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cargar recursos NLP\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    sentence_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "    log_info('Recursos NLP cargados correctamente.')\n",
    "except Exception as e:\n",
    "    log_error(f'Error al cargar stopwords: {e}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39984a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @115712 I understand. I would like to assist y...\n",
       "1              @sprintcare and how do you propose we do that\n",
       "2          @sprintcare I have sent several private messag...\n",
       "3          @115712 Please send us a Private Message so th...\n",
       "4                                         @sprintcare I did.\n",
       "                                 ...                        \n",
       "2811769    @823869 Hey, we'd be happy to look into this f...\n",
       "2811770    @115714 wtf!? I’ve been having really shitty s...\n",
       "2811771    @143549 @sprintcare You have to go to https://...\n",
       "2811772    @823870 Sounds delicious, Sarah! 😋 https://t.c...\n",
       "2811773    @AldiUK  warm sloe gin mince pies with ice cre...\n",
       "Name: text, Length: 2811774, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cst_twcs['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37c154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [@115712 I understand., I would like to assist...\n",
       "1            [@sprintcare and how do you propose we do that]\n",
       "2          [@sprintcare I have sent several private messa...\n",
       "3          [@115712 Please send us a Private Message so t...\n",
       "4                                       [@sprintcare I did.]\n",
       "                                 ...                        \n",
       "2811769    [@823869 Hey, we'd be happy to look into this ...\n",
       "2811770    [@115714 wtf!?, I’ve been having really shitty...\n",
       "2811771    [@143549 @sprintcare You have to go to https:/...\n",
       "2811772    [@823870 Sounds delicious, Sarah!, 😋 https://t...\n",
       "2811773    [@AldiUK  warm sloe gin mince pies with ice cr...\n",
       "Name: tokenized_sentences, Length: 2811774, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicar la tokenización de oraciones a cada fila\n",
    "#data_cst_twcs['tokenized_sentences'] = data_cst_twcs['text'].apply(sentence_tokenizer.tokenize)\n",
    "#data_cst_twcs['tokenized_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72fea96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1377768,)\n",
      "X_valid:  (590473,)\n",
      "X_test:  (843533,)\n",
      "y_train:  (1377768,)\n",
      "y_valid:  (590473,)\n",
      "y_test:  (843533,)\n",
      "Datos divididos en conjuntos de entrenamiento, validación y prueba correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "# División estratificada para mantener la misma proporción de clases en cada conjunto\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cst_twcs['text'], data_cst_twcs['inbound'], \n",
    "                                                        test_size=0.3, stratify=data_cst_twcs['inbound'], \n",
    "                                                        random_state=42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                        test_size=0.3, stratify=y_train, \n",
    "                                                        random_state=42)\n",
    "    print('X_train: ', X_train.shape)\n",
    "    print('X_valid: ', X_valid.shape)\n",
    "    print('X_test: ', X_test.shape)\n",
    "    print('y_train: ', y_train.shape)\n",
    "    print('y_valid: ', y_valid.shape)\n",
    "    print('y_test: ', y_test.shape)\n",
    "    log_info('Datos divididos en conjuntos de entrenamiento, validación y prueba correctamente.')\n",
    "except Exception as e:\n",
    "    log_error(f'Error al dividir los datos: {e}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c0a93b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento ya existe\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment name\n",
    "try:\n",
    "    mlflow.create_experiment(\"experimento_catboost\")\n",
    "    print(\"Experimento creado\")\n",
    "    logging.info(\"Experiment created successfully.\")\n",
    "except:\n",
    "    mlflow.set_experiment(\"experimento_catboost\")\n",
    "    print(\"Experimento ya existe\")\n",
    "    logging.info(\"Experiment already exists, set to existing experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee174f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el experimento...\n",
      "0:\tlearn: 0.6316539\ttotal: 2.51s\tremaining: 20m 52s\n",
      "100:\tlearn: 0.3638261\ttotal: 26.8s\tremaining: 1m 45s\n",
      "200:\tlearn: 0.3455131\ttotal: 51s\tremaining: 1m 15s\n",
      "300:\tlearn: 0.3375544\ttotal: 1m 14s\tremaining: 49s\n",
      "400:\tlearn: 0.3330456\ttotal: 1m 37s\tremaining: 24.2s\n",
      "499:\tlearn: 0.3297846\ttotal: 2m 1s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'modelo_catboost_prueba' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'modelo_catboost_prueba'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo registrado con precisión: 0.8513466574514571\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Iniciando el experimento...\")\n",
    "    with mlflow.start_run():\n",
    "        # Define y entrena el pipeline\n",
    "        catboost_params = {\n",
    "            \"iterations\": 500,\n",
    "            \"depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"verbose\": 100\n",
    "        }\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=100, lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "            ('catboost', CatBoostClassifier(**catboost_params))\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Inferir signature para input/output\n",
    "        signature = infer_signature(X_train, y_train)\n",
    "\n",
    "        # Registra el modelo con un ejemplo de entrada\n",
    "        input_example = np.array(X_train[:1])  # Toma una muestra como ejemplo de entrada\n",
    "        mlflow.sklearn.log_model(pipeline, \"modelo_catboost\", input_example=input_example, signature=signature, registered_model_name=\"modelo_catboost_prueba\")\n",
    "\n",
    "        # Registra las métricas\n",
    "        accuracy = pipeline.score(X_test, y_test)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Registra los hiperparámetros del modelo\n",
    "        mlflow.log_params(catboost_params)\n",
    "\n",
    "        print(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "        logging.info(f\"Modelo registrado con precisión: {accuracy}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during MLflow run: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220013b9",
   "metadata": {},
   "source": [
    "## Implementación del Flujo MLOps\n",
    "    \n",
    "A continuación, implementamos las funciones para:\n",
    "1. Recuperar el modelo campeón actual\n",
    "2. Entrenar un nuevo modelo\n",
    "3. Evaluar y comparar ambos modelos\n",
    "4. Promover el mejor modelo como nuevo campeón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e373674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name='modelo'):\n",
    "    '''Evalúa un modelo y devuelve un diccionario con múltiples métricas.'''\n",
    "    try:\n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "        # Métricas básicas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1': f1_score(y, y_pred),\n",
    "        }\n",
    "            \n",
    "        # Métricas avanzadas si hay probabilidades\n",
    "        if y_pred_proba is not None:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_pred_proba)\n",
    "            metrics['avg_precision'] = average_precision_score(y, y_pred_proba)\n",
    "            \n",
    "        # Reporte de clasificación\n",
    "        report = classification_report(y, y_pred, output_dict=True)\n",
    "            \n",
    "        log_info(f'Evaluación de {model_name}:')\n",
    "        for metric, value in metrics.items():\n",
    "            log_info(f'- {metric}: {value:.4f}')\n",
    "                \n",
    "        return metrics, report\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al evaluar el modelo {model_name}: {e}')\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91d1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X, y, title='Matriz de Confusión'):\n",
    "    '''Grafica la matriz de confusión para un modelo.'''\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ConfusionMatrixDisplay.from_estimator(model, X, y, ax=ax)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al graficar matriz de confusión: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3998367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, X, y, model_name='Modelo'):\n",
    "    '''Grafica la curva ROC para un modelo.'''\n",
    "    try:\n",
    "        if not hasattr(model, 'predict_proba'):\n",
    "            log_info(f'El modelo {model_name} no soporta predict_proba, no se puede graficar ROC.')\n",
    "            return None\n",
    "                \n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Curva ROC')\n",
    "        ax.legend(loc='lower right')\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al graficar curva ROC: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21339c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_model():\n",
    "    '''Recupera el modelo campeón actual desde MLflow.'''\n",
    "    try:\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "            \n",
    "        # Buscar la última versión del modelo campeón\n",
    "        try:\n",
    "            latest_version = client.get_latest_versions(CHAMPION_MODEL_NAME, stages=['Production'])\n",
    "            if not latest_version:\n",
    "                log_info(f'No se encontró un modelo {CHAMPION_MODEL_NAME} en producción. Buscando en staging...')\n",
    "                latest_version = client.get_latest_versions(CHAMPION_MODEL_NAME, stages=['Staging'])\n",
    "                    \n",
    "            if not latest_version:\n",
    "                log_info(f'No se encontró un modelo {CHAMPION_MODEL_NAME} en staging. Buscando la versión más reciente...')\n",
    "                latest_version = client.get_latest_versions(CHAMPION_MODEL_NAME)\n",
    "                    \n",
    "            if latest_version:\n",
    "                model_uri = f'models:/{CHAMPION_MODEL_NAME}/{latest_version[0].version}'\n",
    "                champion_model = mlflow.sklearn.load_model(model_uri)\n",
    "                log_info(f'Modelo campeón cargado: {CHAMPION_MODEL_NAME} version {latest_version[0].version}')\n",
    "                return champion_model, latest_version[0].run_id\n",
    "            else:\n",
    "                log_info(f'No se encontró ningún modelo registrado con el nombre {CHAMPION_MODEL_NAME}')\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            log_error(f'No se pudo obtener la última versión del modelo: {e}')\n",
    "            return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f'Error al recuperar el modelo campeón: {e}')\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec34372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_challenger_model(model, metrics, X_train, y_train, is_champion=False):\n",
    "    '''Registra un modelo desafiante en MLflow.'''\n",
    "    try:\n",
    "        # Inferir firma para input/output\n",
    "        signature = infer_signature(X_train, y_train)\n",
    "            \n",
    "        # Registrar el modelo con un ejemplo de entrada\n",
    "        input_example = np.array(X_train[:1])\n",
    "            \n",
    "        # Nombre del modelo y etapa\n",
    "        model_name = CHAMPION_MODEL_NAME if is_champion else f'{CHAMPION_MODEL_NAME}_challenger'\n",
    "        stage = 'Production' if is_champion else 'Staging'\n",
    "            \n",
    "        # Registrar modelo\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            'model', \n",
    "            input_example=input_example, \n",
    "            signature=signature, \n",
    "            registered_model_name=model_name\n",
    "        )\n",
    "            \n",
    "        # Registrar métricas\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "        # Si es el campeón, mover a producción\n",
    "        if is_champion:\n",
    "            client = mlflow.tracking.MlflowClient()\n",
    "            latest_version = client.get_latest_versions(model_name)[0].version\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=latest_version,\n",
    "                stage=stage\n",
    "            )\n",
    "            log_info(f'Modelo {model_name} v{latest_version} promocionado a {stage}')\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al registrar el modelo: {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630442ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_challenger_model(X_train, y_train, X_valid=None, y_valid=None, hyperparams=None):\n",
    "    '''Entrena un nuevo modelo desafiante.'''\n",
    "    try:\n",
    "        # Parámetros por defecto si no se especifican\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {\n",
    "                'iterations': 500,\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'verbose': 100,\n",
    "                'max_features': 100\n",
    "            }\n",
    "            \n",
    "        # Extraer parámetros específicos de vectorizador y modelo\n",
    "        max_features = hyperparams.pop('max_features', 100)\n",
    "            \n",
    "        log_info(f'Entrenando modelo desafiante con parámetros: {hyperparams}')\n",
    "            \n",
    "        # Crear pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=max_features, lowercase=True, token_pattern=r'\\b\\w+\\b')),              \n",
    "            ('catboost', CatBoostClassifier(**hyperparams))\n",
    "        ])\n",
    "            \n",
    "        # Entrenar modelo\n",
    "        if X_valid is not None and y_valid is not None:\n",
    "            # Usar conjunto de validación para early stopping\n",
    "            pipeline.fit(X_train, y_train, catboost__eval_set=[(X_valid, y_valid)])\n",
    "        else:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "        log_info('Modelo desafiante entrenado correctamente')\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al entrenar modelo desafiante: {e}')\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(X_train, y_train, X_valid, y_valid, max_evals=10):\n",
    "    '''Optimiza hiperparámetros usando Hyperopt.'''\n",
    "    try:\n",
    "        log_info('Iniciando optimización de hiperparámetros...')\n",
    "            \n",
    "        # Definir espacio de búsqueda\n",
    "        space = {\n",
    "            'max_features': hp.choice('max_features', [50, 100, 200, 300]),\n",
    "            'iterations': hp.choice('iterations', [300, 500, 700]),\n",
    "            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),\n",
    "            'depth': hp.choice('depth', [4, 6, 8, 10]),\n",
    "            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', np.log(1), np.log(10)),\n",
    "            'verbose': 0\n",
    "        }\n",
    "            \n",
    "        # Función objetivo para minimizar\n",
    "        def objective(params):\n",
    "            # Extraer max_features para TfidfVectorizer\n",
    "            max_features = params.pop('max_features', 100)\n",
    "                \n",
    "            # Crear y entrenar pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=english_stopwords, max_features=max_features, \n",
    "                                            lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "                ('catboost', CatBoostClassifier(**params))\n",
    "            ])\n",
    "                \n",
    "            try:\n",
    "                pipeline.fit(X_train, y_train, catboost__eval_set=[(X_valid, y_valid)])\n",
    "                    \n",
    "                # Evaluar en conjunto de validación\n",
    "                y_pred = pipeline.predict(X_valid)\n",
    "                f1 = f1_score(y_valid, y_pred)\n",
    "                    \n",
    "                # Reintegrar max_features al diccionario de parámetros\n",
    "                params['max_features'] = max_features\n",
    "                    \n",
    "                return {'loss': -f1, 'status': STATUS_OK, 'params': params}\n",
    "            except Exception as e:\n",
    "                log_error(f'Error en evaluación de hiperparámetros: {e}')\n",
    "                return {'loss': 0, 'status': STATUS_OK, 'params': params}\n",
    "            \n",
    "        # Ejecutar optimización\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "            \n",
    "        # Obtener mejores parámetros\n",
    "        best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "            \n",
    "        log_info(f'Mejores hiperparámetros encontrados: {best_params}')\n",
    "        return best_params\n",
    "    except Exception as e:\n",
    "        log_error(f'Error en optimización de hiperparámetros: {e}')\n",
    "        # Devolver parámetros por defecto en caso de error\n",
    "        return {\n",
    "            'iterations': 500,\n",
    "            'depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'verbose': 100,\n",
    "            'max_features': 100\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fcc78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(champion_metrics, challenger_metrics, primary_metric=PRIMARY_METRIC, threshold=IMPROVEMENT_THRESHOLD):\n",
    "    '''Compara los modelos y determina si el desafiante debe convertirse en el nuevo campeón.'''\n",
    "    try:\n",
    "        if champion_metrics is None:\n",
    "            log_info('No hay modelo campeón para comparar. El desafiante se convierte en campeón por defecto.')\n",
    "            return True, {}, {'champion': None, 'challenger': challenger_metrics[primary_metric]}\n",
    "            \n",
    "        # Comparar métricas primarias\n",
    "        champion_score = champion_metrics[primary_metric]\n",
    "        challenger_score = challenger_metrics[primary_metric]\n",
    "            \n",
    "        improvement = challenger_score - champion_score\n",
    "        percent_improvement = (improvement / champion_score) * 100 if champion_score > 0 else float('inf')\n",
    "            \n",
    "        comparison = {\n",
    "            'champion': champion_score,\n",
    "            'challenger': challenger_score,\n",
    "            'absolute_diff': improvement,\n",
    "            'percent_diff': percent_improvement\n",
    "        }\n",
    "            \n",
    "        # Comparar todas las métricas disponibles\n",
    "        all_metrics = {}\n",
    "        for metric in set(champion_metrics.keys()).union(challenger_metrics.keys()):\n",
    "            if metric in champion_metrics and metric in challenger_metrics:\n",
    "                champion_val = champion_metrics[metric]\n",
    "                challenger_val = challenger_metrics[metric]\n",
    "                diff = challenger_val - champion_val\n",
    "                perc_diff = (diff / champion_val) * 100 if champion_val > 0 else float('inf')\n",
    "                    \n",
    "                all_metrics[metric] = {\n",
    "                    'champion': champion_val,\n",
    "                    'challenger': challenger_val,\n",
    "                    'absolute_diff': diff,\n",
    "                    'percent_diff': perc_diff\n",
    "                }\n",
    "            \n",
    "        # Determinar si el desafiante es mejor\n",
    "        is_better = improvement > threshold\n",
    "            \n",
    "        if is_better:\n",
    "            log_info(f'El modelo desafiante es mejor en {primary_metric}: {challenger_score:.4f} vs {champion_score:.4f} ')\n",
    "            log_info(f'Mejora absoluta: {improvement:.4f}, Mejora porcentual: {percent_improvement:.2f}%')\n",
    "        else:\n",
    "            log_info(f'El modelo desafiante NO supera al campeón en {primary_metric}: {challenger_score:.4f} vs {champion_score:.4f}')\n",
    "            log_info(f'Diferencia absoluta: {improvement:.4f}, Diferencia porcentual: {percent_improvement:.2f}%')\n",
    "                \n",
    "        return is_better, all_metrics, comparison\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al comparar modelos: {e}')\n",
    "        return False, {}, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f88d8f",
   "metadata": {},
   "source": [
    "## Flujo Principal MLOps\n",
    "    \n",
    "A continuación implementamos el flujo completo del proceso MLOps con los siguientes pasos:\n",
    "1. Configurar MLflow\n",
    "2. Obtener el modelo campeón actual\n",
    "3. Optimizar hiperparámetros para el modelo desafiante\n",
    "4. Entrenar el modelo desafiante\n",
    "5. Evaluar ambos modelos\n",
    "6. Comparar modelos y seleccionar el mejor\n",
    "7. Registrar el nuevo campeón si corresponde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a34664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento \"experimento_catboost\" ya existe, usando experimento existente\n"
     ]
    }
   ],
   "source": [
    "# Configurar MLflow\n",
    "try:\n",
    "    # Crear experimento si no existe\n",
    "    try:\n",
    "        mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        log_info(f'Experimento \"{EXPERIMENT_NAME}\" creado correctamente')\n",
    "    except:\n",
    "        mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "        log_info(f'Experimento \"{EXPERIMENT_NAME}\" ya existe, usando experimento existente')\n",
    "except Exception as e:\n",
    "    log_error(f'Error al configurar MLflow: {e}')\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bef3e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PROCESO DE REENTRENAMIENTO MLOPS ===\n",
      "1. Obteniendo modelo campeón...\n",
      "ERROR: No se pudo obtener la última versión del modelo: Registered Model with name=modelo_catboost_champion not found\n",
      "No se encontró modelo campeón existente\n",
      "2. Optimizando hiperparámetros para modelo desafiante...\n",
      "Iniciando optimización de hiperparámetros...\n",
      "ERROR: Error en evaluación de hiperparámetros: 0      \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "ERROR: Error en evaluación de hiperparámetros: 0                 \n",
      "100%|██████████| 10/10 [03:53<00:00, 23.31s/trial, best loss: 0.0]\n",
      "Mejores hiperparámetros encontrados: {'depth': 6, 'iterations': 500, 'l2_leaf_reg': 6.210334752527742, 'learning_rate': 0.12145177637340847, 'verbose': 0}\n",
      "3. Entrenando modelo desafiante...\n",
      "Entrenando modelo desafiante con parámetros: {'depth': 6, 'iterations': 500, 'l2_leaf_reg': 6.210334752527742, 'learning_rate': 0.12145177637340847, 'verbose': 0}\n",
      "ERROR: Error al entrenar modelo desafiante: 0\n",
      "ERROR: Error en flujo MLOps: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     89\u001b[39m     log_error(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mError en flujo MLOps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 3. Entrenar modelo desafiante\u001b[39;00m\n\u001b[32m     36\u001b[39m log_info(\u001b[33m'\u001b[39m\u001b[33m3. Entrenando modelo desafiante...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m challenger_model = \u001b[43mtrain_challenger_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 4. Evaluar ambos modelos en conjunto de prueba\u001b[39;00m\n\u001b[32m     40\u001b[39m log_info(\u001b[33m'\u001b[39m\u001b[33m4. Evaluando modelos en conjunto de prueba...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_challenger_model\u001b[39m\u001b[34m(X_train, y_train, X_valid, y_valid, hyperparams)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     35\u001b[39m     log_error(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mError al entrenar modelo desafiante: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_challenger_model\u001b[39m\u001b[34m(X_train, y_train, X_valid, y_valid, hyperparams)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_valid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_valid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Usar conjunto de validación para early stopping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatboost__eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m     pipeline.fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/catboost/core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/catboost/core.py:2395\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES + (Pool,)):\n\u001b[32m   2393\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m\"\u001b[39m\u001b[33my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2395\u001b[39m train_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_train_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2405\u001b[39m params = train_params[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2406\u001b[39m train_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mtrain_pool\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/catboost/core.py:2348\u001b[39m, in \u001b[36mCatBoost._prepare_train_params\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[39m\n\u001b[32m   2345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m eval_set[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33meval_set\u001b[39m\u001b[33m'\u001b[39m\u001b[33m tuple contains at least one None value\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2347\u001b[39m eval_sets.append(\n\u001b[32m-> \u001b[39m\u001b[32m2348\u001b[39m     \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2349\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2350\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cat_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2353\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_embedding_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2355\u001b[39m )\n\u001b[32m   2357\u001b[39m eval_total_row_count += eval_sets[-\u001b[32m1\u001b[39m].num_row()\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_sets[-\u001b[32m1\u001b[39m].num_row() == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/catboost/core.py:769\u001b[39m, in \u001b[36mPool.__init__\u001b[39m\u001b[34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    768\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_data_type(data)\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_data_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pairs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, PATH_TYPES) != \u001b[38;5;28misinstance\u001b[39m(pairs, PATH_TYPES):\n\u001b[32m    771\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m\"\u001b[39m\u001b[33mdata and pairs parameters should be the same types.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/catboost/core.py:945\u001b[39m, in \u001b[36mPool._check_data_empty\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    943\u001b[39m     data_shape = np.shape(data)\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_shape) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m data_shape[\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, Iterable):\n\u001b[32m    946\u001b[39m         data_shape = \u001b[38;5;28mtuple\u001b[39m(data_shape + \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mlen\u001b[39m(data[\u001b[32m0\u001b[39m])]))\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyectos/MLE_Nequi/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "# Implementar el flujo MLOps completo\n",
    "try:\n",
    "    with mlflow.start_run(run_name=f'reentrenamiento_{datetime.now().strftime('%Y%m%d_%H%M')}') as run:\n",
    "        log_info('=== INICIANDO PROCESO DE REENTRENAMIENTO MLOPS ===')\n",
    "            \n",
    "        # 1. Obtener modelo campeón\n",
    "        log_info('1. Obteniendo modelo campeón...')\n",
    "        champion_model, champion_run_id = get_champion_model()\n",
    "            \n",
    "        if champion_model is not None:\n",
    "            log_info('Modelo campeón cargado correctamente')\n",
    "            # Registrar información del modelo campeón\n",
    "            mlflow.set_tag('champion_run_id', champion_run_id)\n",
    "        else:\n",
    "            log_info('No se encontró modelo campeón existente')\n",
    "            \n",
    "        # 2. Optimizar hiperparámetros (si hay suficientes datos)\n",
    "        if len(X_train) > 1000:  # Solo optimizar si hay suficientes datos\n",
    "            log_info('2. Optimizando hiperparámetros para modelo desafiante...')\n",
    "            best_params = optimize_hyperparameters(X_train, y_train, X_valid, y_valid, max_evals=10)\n",
    "        else:\n",
    "            log_info('No hay suficientes datos para optimización de hiperparámetros, usando valores por defecto')\n",
    "            best_params = {\n",
    "                'iterations': 500,\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'verbose': 100,\n",
    "                'max_features': 100\n",
    "            }\n",
    "            \n",
    "        # Registrar hiperparámetros\n",
    "        for param_name, param_value in best_params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "            \n",
    "        # 3. Entrenar modelo desafiante\n",
    "        log_info('3. Entrenando modelo desafiante...')\n",
    "        challenger_model = train_challenger_model(X_train, y_train, X_valid, y_valid, best_params)\n",
    "            \n",
    "        # 4. Evaluar ambos modelos en conjunto de prueba\n",
    "        log_info('4. Evaluando modelos en conjunto de prueba...')\n",
    "            \n",
    "        if champion_model is not None:\n",
    "            champion_metrics, champion_report = evaluate_model(champion_model, X_test, y_test, 'Modelo Campeón')\n",
    "            # Guardar matriz de confusión y curva ROC\n",
    "            cm_fig_champion = plot_confusion_matrix(champion_model, X_test, y_test, 'Matriz de Confusión - Modelo Campeón')\n",
    "            if cm_fig_champion:\n",
    "                mlflow.log_figure(cm_fig_champion, 'confusion_matrix_champion.png')\n",
    "                    \n",
    "            roc_fig_champion = plot_roc_curve(champion_model, X_test, y_test, 'Modelo Campeón')\n",
    "            if roc_fig_champion:\n",
    "                mlflow.log_figure(roc_fig_champion, 'roc_curve_champion.png')\n",
    "        else:\n",
    "            champion_metrics = None\n",
    "            champion_report = None\n",
    "            \n",
    "        challenger_metrics, challenger_report = evaluate_model(challenger_model, X_test, y_test, 'Modelo Desafiante')\n",
    "            \n",
    "        # Guardar matriz de confusión y curva ROC\n",
    "        cm_fig_challenger = plot_confusion_matrix(challenger_model, X_test, y_test, 'Matriz de Confusión - Modelo Desafiante')\n",
    "        if cm_fig_challenger:\n",
    "            mlflow.log_figure(cm_fig_challenger, 'confusion_matrix_challenger.png')\n",
    "                \n",
    "        roc_fig_challenger = plot_roc_curve(challenger_model, X_test, y_test, 'Modelo Desafiante')\n",
    "        if roc_fig_challenger:\n",
    "            mlflow.log_figure(roc_fig_challenger, 'roc_curve_challenger.png')\n",
    "            \n",
    "        # 5. Comparar modelos\n",
    "        log_info('5. Comparando modelos...')\n",
    "        is_challenger_better, all_metrics_comparison, primary_comparison = compare_models(\n",
    "            champion_metrics, challenger_metrics, PRIMARY_METRIC, IMPROVEMENT_THRESHOLD\n",
    "        )\n",
    "            \n",
    "        # Guardar comparaciones\n",
    "        mlflow.log_dict(all_metrics_comparison, 'metrics_comparison.json')\n",
    "        mlflow.log_dict(primary_comparison, f'{PRIMARY_METRIC}_comparison.json')\n",
    "            \n",
    "        # 6. Registrar modelo ganador\n",
    "        if is_challenger_better:\n",
    "            log_info('6. El modelo desafiante es mejor. Promoviendo a nuevo campeón...')\n",
    "            register_challenger_model(challenger_model, challenger_metrics, X_train, y_train, is_champion=True)\n",
    "            mlflow.log_param('winner', 'challenger')\n",
    "        else:\n",
    "            log_info('6. El modelo campeón sigue siendo mejor. Registrando desafiante para referencia...')\n",
    "            register_challenger_model(challenger_model, challenger_metrics, X_train, y_train, is_champion=False)\n",
    "            mlflow.log_param('winner', 'champion')\n",
    "            \n",
    "        log_info('=== PROCESO MLOps COMPLETADO EXITOSAMENTE ===')\n",
    "except Exception as e:\n",
    "    log_error(f'Error en flujo MLOps: {e}')\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3da79",
   "metadata": {},
   "source": [
    "## Visualizaciones de Resultados\n",
    "    \n",
    "A continuación, se muestran algunas visualizaciones adicionales para analizar y comparar los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4753300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(champion_metrics, challenger_metrics):\n",
    "    '''Genera gráficos comparativos de métricas entre el modelo campeón y el desafiante.'''\n",
    "    try:\n",
    "        if champion_metrics is None:\n",
    "            log_info('No hay métricas del modelo campeón para comparar.')\n",
    "            return None\n",
    "                \n",
    "        # Encontrar métricas comunes\n",
    "        common_metrics = set(champion_metrics.keys()).intersection(set(challenger_metrics.keys()))\n",
    "            \n",
    "        # Preparar datos para ploteo\n",
    "        metrics_names = list(common_metrics)\n",
    "        champion_values = [champion_metrics[metric] for metric in metrics_names]\n",
    "        challenger_values = [challenger_metrics[metric] for metric in metrics_names]\n",
    "            \n",
    "        # Crear figura\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "        # Configuración de barras\n",
    "        x = np.arange(len(metrics_names))\n",
    "        width = 0.35\n",
    "            \n",
    "        # Graficar barras\n",
    "        ax.bar(x - width/2, champion_values, width, label='Modelo Campeón')\n",
    "        ax.bar(x + width/2, challenger_values, width, label='Modelo Desafiante')\n",
    "            \n",
    "        # Añadir etiquetas y título\n",
    "        ax.set_xlabel('Métricas')\n",
    "        ax.set_ylabel('Valor')\n",
    "        ax.set_title('Comparación de Métricas entre Modelos')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics_names)\n",
    "        ax.legend()\n",
    "            \n",
    "        # Añadir valores encima de cada barra\n",
    "        for i, v in enumerate(champion_values):\n",
    "            ax.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "                \n",
    "        for i, v in enumerate(challenger_values):\n",
    "            ax.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        log_error(f'Error al generar gráfico comparativo: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparación de métricas si hay dos modelos para comparar\n",
    "if champion_metrics is not None and challenger_metrics is not None:\n",
    "    comparison_fig = plot_metrics_comparison(champion_metrics, challenger_metrics)\n",
    "    if comparison_fig:\n",
    "        # Mostrar en notebook\n",
    "        plt.show()\n",
    "        # Guardar en MLflow\n",
    "        with mlflow.start_run(run_id=run.info.run_id):\n",
    "            mlflow.log_figure(comparison_fig, 'metrics_comparison.png')\n",
    "else:\n",
    "    print('No hay dos modelos para comparar métricas.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee390e2",
   "metadata": {},
   "source": [
    "## Automatización del Proceso MLOps\n",
    "    \n",
    "Para integrar este notebook en un flujo de trabajo automatizado MLOps, se puede exportar como script Python y programar su ejecución periódica usando herramientas como Apache Airflow, cron jobs, o plataformas CI/CD. A continuación se muestra cómo configurar los metadatos del experimento para facilitar el seguimiento.\n",
    "    \n",
    "Ejemplo de configuración para Airflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este código no se ejecuta aquí, es solo un ejemplo de DAG para Airflow\n",
    "'''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "    \n",
    "default_args = {\n",
    "    'owner': 'mlops',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "    \n",
    "def run_retraining():\n",
    "    # Ejecutar script de reentrenamiento\n",
    "    subprocess.run(['python', 'reentrenamiento_mlops.py'])\n",
    "    \n",
    "with DAG(\n",
    "    'modelo_catboost_retraining',\n",
    "    default_args=default_args,\n",
    "    description='Reentrenamiento periódico del modelo de clasificación',\n",
    "    schedule_interval=timedelta(days=7),  # Reentrenar semanalmente\n",
    "    start_date=datetime(2025, 5, 1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    retraining_task = PythonOperator(\n",
    "        task_id='reentrenamiento_modelo',\n",
    "        python_callable=run_retraining,\n",
    "    )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff3189",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "    \n",
    "Este notebook implementa un flujo MLOps completo para:\n",
    "    \n",
    "1. **Entrenamiento automatizado**: Entrenamiento automatizado del modelo utilizando los últimos datos disponibles.\n",
    "2. **Optimización de hiperparámetros**: Búsqueda automática de los mejores hiperparámetros para el modelo.\n",
    "3. **Gestión de modelos**: Uso de MLflow para registrar y versionar modelos.\n",
    "4. **Evaluación comparativa**: Comparación sistemática entre modelo campeón y desafiante.\n",
    "5. **Promoción automática**: Promoción automática del mejor modelo a producción basado en métricas objetivas.\n",
    "    \n",
    "Este enfoque garantiza que solo los modelos que realmente mejoran el rendimiento sean promovidos a producción, manteniendo un historial completo de experimentos y decisiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para exportar este notebook como script Python (opcional)\n",
    "try:\n",
    "    !jupyter nbconvert --to python reentrenamiento_mlops.ipynb\n",
    "    print('Notebook exportado como script Python: reentrenamiento_mlops.py')\n",
    "except Exception as e:\n",
    "    print(f'Error al exportar notebook: {e}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
